# attention is all you need

- self attention
- multi-head attention，作为一个self-attention的扩展机制
- 使用位置编码表示序列的顺序
- 解码模块

编码组件
解码组件
中间连接部分

## self attention

### 宏观

当前单词的解释也依赖与整个句子的所有内容

### 微观:详细介绍整个self-attention机制的实现方式

1. 首先是通过映射矩阵的乘积，将每个词向量映射出维度较低的查询向量q、键向量k、值向量v
2. 单个具体的词的q与每个词的k向量做点积和softmax作为每个词对当前词的贡献x
3. 对于每个位置的贡献x与当前位置的v的乘积之和作为新的词向量Z

## transformer

transformer表示的是一种类似于所述结构的一种框架，在目前的应用场景中主要是进行的***文本翻译***的训练，包含encoder和decoder且去***除掉传统的rnn***的一种神经网络结构

## 解码组件的实现



## 存在的问题详细解释

- 在encoding的部分，每个自注意力层的后面会去接一个feed-forward NN，那么对于现在每个句子来说，词列表的长度是固定的，那么每个词进入以及反向梯度传播的是否是同一个feed-forward，还是说每个位置的都不同？

  **答：**目前结果来看是同一个，且这个是可以进行并行的处理，这也是在文章中提到说self-attention不可并行，但是在feed-foreard阶段可以并行，但是具体的实现还是得看代码怎么做的

- 我们可以注意到在self-attention的机制中，最初的词向量和得到的Z的词向量的维度并不相同？

  **答：**这正是Multi Headed的机制的实现，即用多个self-attention来对同一个词向量进行处理，然后再进行concate和乘以矩阵，这样的方式来完成对词向量的多种子空间的合并

- 

注意点：
	encoding 阶段的每个词都有自己的独特路径，因为前面是一个连续的过程
		但是这里对于self attention部分是不可并行的，因为存在互相依赖，但是对于feed-forward阶段则是可以并行的，因为并不会互相影响和依赖

依赖详细解释

​		猜测：是不是一个feed-forward其实只是针对单个词的512维度的共用，对于所有的词都是一个网络，但是self attention则不是
​	multi-head：
​		其实multi-head就是一个很简单的运用多个同样的self-attention，然后进行concate，再乘一个矩阵回到需要的维度
​		那么为什么多个head就会比单个head好呢？
​	位置也需要作为一种特征的输入，则如何整合进当前的结构中去呢？
​		给到了一种特别奇妙的位置编码，主要是这个编码能够解决变长句子的输入问题
​	层-归一化的步骤：
​		表示的是在encoding的时候，会在self-attention和feed-forward都会给自己增加一个单独的残差-归一化结构
​	解码器
​		解码器的方式始终就是以最后一层encoding的输出的K/V和上一个单词的Q作为整个的输入，而对于输出也是某一个维度的词的向量
​		而且解码器是一个顺序的过程，对于词是一个一个产生的，不是并行的直接产生整个句子的结果
​	linear和softmax：
​		这个就是对整个解码器的输出的过程映射到整个最后输出词的个数维度上去，再根据softmax来确定唯一的词

​	

