# attention is all you need

- self attention
- multi-head attention，作为一个self-attention的扩展机制
- 使用位置编码表示序列的顺序
- 解码模块

编码组件
解码组件
中间连接部分

## self attention

### 宏观

当前单词的解释也依赖与整个句子的所有内容

### 微观:详细介绍整个self-attention机制的实现方式

1. 首先是通过映射矩阵的乘积，将每个词向量映射出维度较低的查询向量q、键向量k、值向量v
2. 单个具体的词的q与每个词的k向量做点积和softmax作为每个词对当前词的贡献x
3. 对于每个位置的贡献x与当前位置的v的乘积之和作为新的词向量Z

## transformer

transformer表示的是一种类似于所述结构的一种框架，在目前的应用场景中主要是进行的***文本翻译***的训练，包含encoder和decoder且去***除掉传统的rnn***的一种神经网络结构

## 解码组件的实现

解码组件的实现其实是一个类似RNN的不断迭代的过程的，每次会把自己前一个位置的词向量经过一次带mask的self-attention，然后作为单独的q，与前面编码部分出来的k和v做混合的self-attention，得到自己的新词向量，然后做linear和softmax,最终one-hot形式的具体词，再将该词作为下一次迭代的基础，直到生成结束符号。我们看到对与编码组件引用之前会有一个解码组件自己做的masked self-attention,这就是解码组件对与当前结果的一个self-attention

## 存在的问题详细解释

- 在encoding的部分，每个自注意力层的后面会去接一个feed-forward NN，那么对于现在每个句子来说，词列表的长度是固定的，那么每个词进入以及反向梯度传播的是否是同一个feed-forward，还是说每个位置的都不同？

  **答：**目前结果来看是同一个，且这个是可以进行并行的处理，这也是在文章中提到说self-attention不可并行，但是在feed-foreard阶段可以并行，但是具体的实现还是得看代码怎么做的

- 我们可以注意到在self-attention的机制中，最初的词向量和得到的Z的词向量的维度并不相同？

  **答：**这正是Multi Headed的机制的实现，即用多个self-attention来对同一个词向量进行处理，然后再进行concate和乘以矩阵，这样的方式来完成对词向量的多种子空间的合并

- Positional Encoding是如何对应不同长度的句子的，尤其是比训练时的句子更长的？

  **答：**首先positional encoding是一个将位置信息进行编码，然后增加到训练输入中的，作为self-attention的补充，因为self-attention本质上也就是一个精妙的“词袋模型“而已。那么这里用了一个sin和cos的函数，作为一个训练位置的映射或者构造公式。这个相对于直接用位置+训练的好处在于能支持更长的句子，且提供了保留相对位置的可能性，因为sin(a+b)是能通过sin的进行线性变换得到的。对于如何将该encoding整合到整个词向量中去，目前有两种方法，*拼接*或者*相加*

- 残差模块，在self-attention和feed-forward两个部分都会有Add&Normalize阶段，这个具体实现，甚至可以和BN层进行一个对比

  **答：**这里的残差模块，就是一个LayerNorm(X+Z)的过程，将前面的输入值X和该层的输出值Z进行一个相加，然后归一化。残差结构是为了解决梯度爆炸和梯度衰减的问题，归一化可以认为是解决每一批数据的分布各不相同而导致的梯度变化比较大的情况，这样加快训练收敛速度，降低过拟合。这个归一化层就如同CNN里面的BN层一样的效果

- self-attention是否可训练，能否简述一下推导的过程？

  **答：**

# BERT

​	

